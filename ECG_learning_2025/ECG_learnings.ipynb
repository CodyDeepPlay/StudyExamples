{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0abff6-22be-4d7b-8954-3484227c1853",
   "metadata": {},
   "source": [
    "# ECG segmentation with Physionet dataset\n",
    "This file is showing an example of how to load the ECG data from the physionet dataset that was originally downloaded from https://www.physionet.org/content/qtdb/1.0.0/\n",
    "\n",
    "This is an exercise of refactorize my code developed couple of years ago.\n",
    "\n",
    "This file contains using different machine learning methods to conduct signal segmentation.\n",
    "\n",
    "\n",
    "Make sure to refer to 'requirements.txt' to use the packages with the listed version. With some other versions of packages, it may not be compatible with 'wfdb' package, most likely the newer version of'numpy' package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f83bf5-1abd-4c89-96c3-29bec98f1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wfdb  # waveform database package, a library of tools for reading, writing, and processing WFDB signals and annotations.\n",
    "\n",
    "from lib import HelpferFunctions as hf  # custom module with helperfunctions needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b573f0-a0e4-4d7a-a114-f37153847f91",
   "metadata": {},
   "source": [
    "### Reorganize data files\n",
    "Each data file contains recordings from two channels, these two channels are ECG recordings collected at the same time, and should be two different recording electrodes. Each data file may include different length of samples. \n",
    "\n",
    "Thus, here, we figure out what is the minimum length of a recording, and truncate all the data using this minimum length and reorganize data into a big single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1b91f1-35f8-49aa-8337-0fccdb583e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of recording files are: 105\n",
      "Minimum signal length is: 224993\n"
     ]
    }
   ],
   "source": [
    "#%% load all the files\n",
    "data_path   = 'qt-database-1.0.0'  # this cardiac data was downloaded and saved locally\n",
    "all_file_names = os.listdir(data_path)\n",
    "\n",
    "min_sig_len = None\n",
    "all_pu1s =[] # get all the pu1 files\n",
    "\n",
    "for a_file in all_file_names:\n",
    "    if 'pu1' in a_file:\n",
    "        a_file_name=a_file.split('.')       \n",
    "        record_name = a_file_name[0] # just the file name, not including extension\n",
    "        \n",
    "        if record_name not in all_pu1s:  # make sure not dupilcated names, so one file will not be added twice\n",
    "            all_pu1s.append(record_name) # only add the file name, not extension\n",
    "            record_path = os.path.join(data_path, record_name)\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            signals, fields = wfdb.rdsamp(record_path)\n",
    "            fs = fields['fs']\n",
    "            length = fields['sig_len']\n",
    "            \n",
    "            if min_sig_len is not None:\n",
    "                min_sig_len = min(min_sig_len, length)\n",
    "            else:                \n",
    "                min_sig_len = length\n",
    "\n",
    "print(\"Total number of recording files are:\", len(all_pu1s))\n",
    "print(\"Minimum signal length is:\" , min_sig_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c973a-f500-4c5d-be07-33762f97e410",
   "metadata": {},
   "source": [
    "Now reorganize separate recordings into single data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba41bdeb-b92f-44eb-a7cf-aa64225f449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file shape is [number of observations, time stamps, number of features]\n",
      "(105, 224993, 2)\n",
      "Annotation file shape is [number of observations, time stamps]\n",
      "(105, 224993)\n",
      "Reshape annotation file to meet the requirement of the lstm model input\n",
      "(105, 224993, 1)\n"
     ]
    }
   ],
   "source": [
    "All_data_list = []  # create a space to host all the data\n",
    "All_annotation_list = [] # host all the ECG signal annotation\n",
    "\n",
    "for record_name in all_pu1s:\n",
    "    record_path = os.path.join(data_path, record_name)\n",
    "    record = wfdb.rdrecord(record_path)        # read a single file\n",
    "    signals, fields = wfdb.rdsamp(record_path) # extract the signals from a single file\n",
    "   \n",
    "    # read the annotation for a single file\n",
    "    annotation   = wfdb.rdann(record_path, 'pu1')\n",
    "    annot_expand = hf.expand_annotation(annotation.sample, annotation.symbol, length)\n",
    "    \n",
    "    # use the smallest signal length to truncate all the data, so that \n",
    "    # we can contatenate all the signal recordings in a single large dataset\n",
    "    signals2add = signals[0:min_sig_len,:]\n",
    "    annotation2add = annot_expand[0:min_sig_len]\n",
    "    \n",
    "    All_data_list.append(signals2add)\n",
    "    All_annotation_list.append(annotation2add)\n",
    "    \n",
    "All_data = np.asarray(All_data_list)  # size(num_obs, time_stamps, n_features)\n",
    "All_annotation = np.asarray(All_annotation_list)  # size(num_obs, time_stamps)\n",
    "\n",
    "print(\"Data file shape is [number of observations, time stamps, number of features]\")\n",
    "print(All_data.shape)\n",
    "print(\"Annotation file shape is [number of observations, time stamps]\")\n",
    "print(All_annotation.shape)\n",
    "\n",
    "All_labels = All_annotation.reshape(All_annotation.shape[0], All_annotation.shape[1], 1)\n",
    "print(\"Reshape annotation file to meet the requirement of the lstm model input\")\n",
    "print(All_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed798b-e8a0-44bc-9c19-46434c534964",
   "metadata": {},
   "source": [
    "## Use LSTM layer\n",
    "Decode the point-wise output of the ECG signal, so to achieve the segmentation of the overall signal.\n",
    "\n",
    "You might have problem install tensorflow on Mac M1 machine, \n",
    "Below are the steps I followed:\n",
    "#### 1.create a new conda environment:\n",
    "conda create -n tensorflow-env python=3.10\n",
    "conda activate tensorflow-env\n",
    "#### 2.install the Apple tensorflow dependencies:\n",
    "conda install -c apple tensorflow-deps\n",
    "#### 3.insall tensorflow for macOS:\n",
    "pip install tensorflow-macos\n",
    "#### 4.install the metal plugin for GPU support:\n",
    "pip install tensorflow-metal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292119e-11ae-457b-80b0-2f5a6637c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf   # tf.__version__ : '2.0.0'\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195f88d-5a2c-4d11-9c35-2d991b33eed8",
   "metadata": {},
   "source": [
    "split the data into 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad22aa7-2ec3-40eb-b800-e0f03d851a4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(all_data, all_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# 20% of the total data left as testing data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# get some data within training data, to be used as validation during training.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_labels, test_size=0.2)  # 20% of the total data left as testing data\n",
    "    \n",
    "# get some data within training data, to be used as validation during training.\n",
    "num_validation = 100\n",
    "start_index    = 500\n",
    "mask = range(start_index, start_index + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4c723-0884-48c5-8be5-a1c6f572e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
