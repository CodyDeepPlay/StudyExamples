{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0abff6-22be-4d7b-8954-3484227c1853",
   "metadata": {},
   "source": [
    "# load all long ECG recordings, and indivisual ECG recordings from the original Physionet dataset\n",
    "This file is showing an example of how to load the ECG data from the physionet dataset that was originally downloaded from https://www.physionet.org/content/qtdb/1.0.0/\n",
    "\n",
    "This is an exercise of refactorize my code developed couple of years ago.\n",
    "\n",
    "This file contains using different machine learning methods to conduct signal segmentation.\n",
    "\n",
    "\n",
    "Make sure to refer to 'requirements.txt' to use the packages with the listed version. With some other versions of packages, it may not be compatible with 'wfdb' package, most likely the newer version of'numpy' package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f83bf5-1abd-4c89-96c3-29bec98f1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wfdb  # waveform database package, a library of tools for reading, writing, and processing WFDB signals and annotations.\n",
    "import pickle\n",
    "\n",
    "from lib import HelpferFunctions as hf  # custom module with helperfunctions needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b573f0-a0e4-4d7a-a114-f37153847f91",
   "metadata": {},
   "source": [
    "### Reorganize data files\n",
    "Each data file contains recordings from two channels, these two channels are ECG recordings collected at the same time, and should be two different recording electrodes. Each data file may include different length of samples. \n",
    "\n",
    "Thus, here, we figure out what is the minimum length of a recording, and truncate all the data using this minimum length and reorganize data into a big single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1b91f1-35f8-49aa-8337-0fccdb583e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of recording files are: 105\n",
      "Minimum signal length is: 224993\n"
     ]
    }
   ],
   "source": [
    "#%% load all the files\n",
    "data_path   = 'qt-database-1.0.0'  # this cardiac data was downloaded and saved locally\n",
    "all_file_names = os.listdir(data_path)\n",
    "\n",
    "min_sig_len = None\n",
    "all_pu1s =[] # get all the pu1 files\n",
    "\n",
    "for a_file in all_file_names:\n",
    "    if 'pu1' in a_file:\n",
    "        a_file_name=a_file.split('.')       \n",
    "        record_name = a_file_name[0] # just the file name, not including extension\n",
    "        \n",
    "        if record_name not in all_pu1s:  # make sure not dupilcated names, so one file will not be added twice\n",
    "            all_pu1s.append(record_name) # only add the file name, not extension\n",
    "            record_path = os.path.join(data_path, record_name)\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            signals, fields = wfdb.rdsamp(record_path)\n",
    "            fs = fields['fs']\n",
    "            length = fields['sig_len']\n",
    "            \n",
    "            if min_sig_len is not None:\n",
    "                min_sig_len = min(min_sig_len, length)\n",
    "            else:                \n",
    "                min_sig_len = length\n",
    "\n",
    "print(\"Total number of recording files are:\", len(all_pu1s))\n",
    "print(\"Minimum signal length is:\" , min_sig_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c973a-f500-4c5d-be07-33762f97e410",
   "metadata": {},
   "source": [
    "Now reorganize separate recordings into single data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba41bdeb-b92f-44eb-a7cf-aa64225f449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file shape is [number of observations, time stamps, number of features]\n",
      "(105, 224993, 2)\n",
      "Annotation file shape is [number of observations, time stamps]\n",
      "(105, 224993)\n",
      "Reshape annotation file to meet the requirement of the lstm model input\n",
      "(105, 224993, 1)\n"
     ]
    }
   ],
   "source": [
    "All_data_list = []  # create a space to host all the data\n",
    "All_annotation_sample = []\n",
    "All_annotation_symbol = []\n",
    "All_annotation_list = [] # host all the ECG signal annotation\n",
    "\n",
    "for record_name in all_pu1s:\n",
    "    record_path = os.path.join(data_path, record_name)\n",
    "    record = wfdb.rdrecord(record_path)        # read a single file\n",
    "    signals, fields = wfdb.rdsamp(record_path) # extract the signals from a single file\n",
    "   \n",
    "    # read the annotation for a single file\n",
    "    annotation   = wfdb.rdann(record_path, 'pu1')\n",
    "    annot_expand = hf.expand_annotation(annotation.sample, annotation.symbol, length)\n",
    "    \n",
    "    # use the smallest signal length to truncate all the data, so that \n",
    "    # we can contatenate all the signal recordings in a single large dataset\n",
    "    signals2add = signals[0:min_sig_len,:]\n",
    "    annotation2add = annot_expand[0:min_sig_len]\n",
    "    \n",
    "    All_data_list.append(signals2add)\n",
    "    All_annotation_sample.append(annotation.sample)\n",
    "    All_annotation_symbol.append(annotation.symbol)\n",
    "    All_annotation_list.append(annotation2add)\n",
    "    \n",
    "All_data = np.asarray(All_data_list)  # size(num_obs, time_stamps, n_features)\n",
    "All_annotation = np.asarray(All_annotation_list)  # size(num_obs, time_stamps)\n",
    "\n",
    "print(\"Data file shape is [number of observations, time stamps, number of features]\")\n",
    "print(All_data.shape)\n",
    "print(\"Annotation file shape is [number of observations, time stamps]\")\n",
    "print(All_annotation.shape)\n",
    "\n",
    "All_labels = All_annotation.reshape(All_annotation.shape[0], All_annotation.shape[1], 1)\n",
    "print(\"Reshape annotation file to meet the requirement of the lstm model input\")\n",
    "print(All_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145022f0-d30c-4a55-87d3-4698c1fe3bea",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f48736-d27d-4ffe-aa79-e90ea58827fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/long_EKG_recording.pkl' already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Example data to save\n",
    "data = {\n",
    "    \"ECG\": {'data': All_data, \n",
    "            'explain': 'data size is with [105, 224993, 2], 105 data files, with each recording length is 224993, and each recording has 2 channel of ECG',},\n",
    "    \"fs\": fs,\n",
    "    \"All_annotation\": {'data': All_annotation,\n",
    "                       'explain': 'converted annotation of all data points. -1: baseline, 0: N, 1: st, 2:t, 3:iso, 4:p, 5:pq'},\n",
    "    \"All_annotation_sample\": {'data': All_annotation_sample,\n",
    "                              'explain': 'original annoation sample for all recordings files'},\n",
    "    \"All_annotation_symbol\": {'data': All_annotation_symbol,\n",
    "                              'explain': 'original annoation symbol for all recordings files'},\n",
    "}\n",
    "\n",
    "# File name for the pickle file\n",
    "filename = \"long_EKG_recording.pkl\"\n",
    "file_path = os.path.join(\"data\", filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the data to the pickle file\n",
    "if os.path.exists(file_path):\n",
    "        print(f\"File '{file_path}' already exists. Skipping save.\")\n",
    "else:\n",
    "    try:\n",
    "        with open(file_path, \"wb\") as file:  # \"wb\" mode for writing binary data\n",
    "            pickle.dump(data, file)\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")#%%\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad188e-f24b-4f9e-b441-6b466b44aab6",
   "metadata": {},
   "source": [
    "#### After saving the data, load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeee4253-92d4-42db-900c-a2de374952e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: dict_keys(['ECG', 'fs', 'all_annotation'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"rb\") as file: # \"rb\" mode for reading binary data\n",
    "        loaded_data = pickle.load(file)\n",
    "    print(\"Loaded data:\", loaded_data.keys())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {filename} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occured during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60380ae-2b4e-4910-949e-68abe27c229e",
   "metadata": {},
   "source": [
    "#### Save the long ECG recording files into individual ECG recordings,\n",
    "Use a custom funtion seg_single_ECGs() to seprate individual ECG signals from the long recordings, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babe6385-a980-4d31-a916-e665edf4c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  save all long ECG file into smaller/individual ECG files\n",
    "\n",
    "all_ECG_data = data['ECG']['data']\n",
    "all_annotation_expand = data['All_annotation']['data']\n",
    "All_annotation_sample = data['All_annotation_sample']['data']\n",
    "All_annotation_symbol = data['All_annotation_symbol']['data']\n",
    "\n",
    "\n",
    "n = len(all_ECG_data) # the total number of ECG recording files\n",
    "\n",
    "# initialize the object to hold data in each iteration of the long ECG recording file.\n",
    "all_ECG_array = None\n",
    "all_annotation = None\n",
    "\n",
    "# iterate through each long ECG long recordings\n",
    "for i in range(n):\n",
    "    my_signal_all_chs = all_ECG_data[i]  # all channels ECG signals for a single recording example\n",
    "    \n",
    "    one_anntype = All_annotation_symbol[i]\n",
    "    one_annsamp = All_annotation_sample[i]\n",
    "    one_expand  = all_annotation_expand[i]\n",
    "    \n",
    "    # the first channel ECG data\n",
    "    my_signal1 = my_signal_all_chs[:, 0]\n",
    "    (single_ECG_list1, single_annotation_list1) = hf.seg_single_ECGs(my_signal1, one_anntype, one_annsamp, one_expand)\n",
    "    # the 2nd channel ECG data\n",
    "    my_signal2 = my_signal_all_chs[:, 1]\n",
    "    (single_ECG_list2, single_annotation_list2) = hf.seg_single_ECGs(my_signal2, one_anntype, one_annsamp, one_expand)\n",
    "    ''' single_annotation_list1 and single_annotation_list2 should be the same, \n",
    "        as they started the same annotation for long ECG recordings. \n",
    "    '''\n",
    "    \n",
    "    # convert all the chs of single ECG signals into one big file,\n",
    "    # size (num_obs, time_length, chs)\n",
    "    single_ECG_array1 = np.asarray(single_ECG_list1)[:,:,np.newaxis]\n",
    "    single_ECG_array2 = np.asarray(single_ECG_list2)[:,:,np.newaxis]\n",
    "    single_ECG_array = np.concatenate((single_ECG_array1, single_ECG_array2),axis=2)\n",
    " \n",
    "    # size (num_obs, time_length)\n",
    "    single_annotation = np.asarray(single_annotation_list2)\n",
    "    \n",
    "    ## concatenate the single ECG file and annotation file into larger file with more observations\n",
    "    if all_ECG_array is None: all_ECG_array = single_ECG_array\n",
    "    else: all_ECG_array = np.concatenate((all_ECG_array, single_ECG_array),axis=0) # add more observations extracted from this new long recording file\n",
    "        \n",
    "    if all_annotation is None: all_annotation = single_annotation\n",
    "    else: all_annotation = np.concatenate((all_annotation, single_annotation),axis=0) # add more observations extracted from this new long recording file\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "498fdfe5-0c22-4785-8775-1019d9c2056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file shape is [number of observations, time stamps, number of features]\n",
      "(111167, 140, 2)\n",
      "Annotation file shape is [number of observations, time stamps]\n",
      "(111167, 140)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data file shape is [number of observations, time stamps, number of features]\")\n",
    "print(all_ECG_array.shape)\n",
    "print(\"Annotation file shape is [number of observations, time stamps]\")\n",
    "print(all_annotation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6394b788-7324-491f-8f10-f4d9d1f1df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/individual_EKG_recording.pkl' already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Example data to save\n",
    "data = {\n",
    "    \"ECG\": {'data': all_ECG_array, \n",
    "            'explain': 'data size is with [111167, 140, 2], 111167 individual ECG recordings, with each recording length is 140, and each recording has 2 channel of ECG',},\n",
    "    \"fs\": fs,\n",
    "    \"all_annotation\": {'data': all_annotation,\n",
    "                       'explain': 'converted annotation of all data points for each individual ECG. -1: baseline, 0: N, 1: st, 2:t, 3:iso, 4:p, 5:pq'},\n",
    "}\n",
    "\n",
    "# File name for the pickle file\n",
    "filename = \"individual_EKG_recording.pkl\"\n",
    "file_path = os.path.join(\"data\", filename)\n",
    "\n",
    "# Save the data to the pickle file\n",
    "if os.path.exists(file_path):\n",
    "        print(f\"File '{file_path}' already exists. Skipping save.\")\n",
    "else:\n",
    "    try:\n",
    "        with open(file_path, \"wb\") as file:  # \"wb\" mode for writing binary data\n",
    "            pickle.dump(data, file)\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b558dcf-1746-4cc5-8fbf-68f7eeaa7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: dict_keys(['ECG', 'fs', 'all_annotation'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"rb\") as file: # \"rb\" mode for reading binary data\n",
    "        individual_data = pickle.load(file)\n",
    "    print(\"Loaded data:\", individual_data.keys())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {filename} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occured during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed798b-e8a0-44bc-9c19-46434c534964",
   "metadata": {},
   "source": [
    "## Use LSTM layer\n",
    "Decode the point-wise output of the ECG signal, so to achieve the segmentation of the overall signal.\n",
    "\n",
    "You might have problem install tensorflow on Mac M1 machine, \n",
    "Below are the steps I followed:\n",
    "#### 1.create a new conda environment:\n",
    "conda create -n tensorflow-env python=3.10\n",
    "conda activate tensorflow-env\n",
    "#### 2.install the Apple tensorflow dependencies:\n",
    "conda install -c apple tensorflow-deps\n",
    "#### 3.insall tensorflow for macOS:\n",
    "pip install tensorflow-macos\n",
    "#### 4.install the metal plugin for GPU support:\n",
    "pip install tensorflow-metal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938a581-1097-4c53-b5b6-6f7bfc6736ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a6b7b-70ae-477b-96eb-aae9d60bf552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d292119e-11ae-457b-80b0-2f5a6637c4b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m   \u001b[38;5;66;03m# tf.__version__ : '2.0.0'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf   # tf.__version__ : '2.0.0'\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195f88d-5a2c-4d11-9c35-2d991b33eed8",
   "metadata": {},
   "source": [
    "split the data into 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad22aa7-2ec3-40eb-b800-e0f03d851a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_labels, test_size=0.2)  # 20% of the total data left as testing data\n",
    "    \n",
    "# get some data within training data, to be used as validation during training.\n",
    "num_validation = 100\n",
    "start_index    = 500\n",
    "mask = range(start_index, start_index + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4c723-0884-48c5-8be5-a1c6f572e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
